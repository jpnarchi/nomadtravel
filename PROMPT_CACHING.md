# Prompt Caching Implementation - Anthropic

## ‚úÖ Implementaci√≥n Actual

Se ha implementado la estructura base para Prompt Caching de Anthropic:

### 1. **SDK Instalado**
- `@anthropic-ai/sdk` instalado y configurado
- Cliente nativo de Anthropic disponible en `anthropicClient`

### 2. **Prompt Refactorizado**
- Funci√≥n `buildSystemPromptWithCache()` separa el prompt en:
  - **Parte est√°tica (~2300 tokens)**: Instrucciones, reglas, workflow - `cache_control: { type: "ephemeral" }`
  - **Parte din√°mica (~200 tokens)**: Usuario, fecha, contexto espec√≠fico

### 3. **Cambios de Modelo**
- **Chat principal**: Cambi√≥ de `openrouter('anthropic/claude-haiku-4.5')` a `anthropic('claude-sonnet-4-20250514')`
- **webSearch**: Ya usaba `anthropic('claude-sonnet-4-20250514')`
- **readAttachment**: Ya usaba `anthropic('claude-sonnet-4-20250514')`

## üîÑ C√≥mo Activar Caching Completo

### Opci√≥n A: Migrar a SDK Nativo (Recomendado para m√°ximo ahorro)

Reemplazar `streamText()` del AI SDK por llamada directa al SDK nativo en `/convex/http.ts`:

```typescript
// En lugar de:
const result = streamText({
    model: anthropic('claude-sonnet-4-20250514'),
    system: systemPrompt,
    messages: convertToModelMessages(messages),
    tools: { ... }
});

// Usar:
const stream = await anthropicClient.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 64_000,
    system: buildSystemPromptWithCache({...}), // Retorna array con cache_control
    messages: convertMessagesToAnthropicFormat(messages),
    tools: convertToolsToAnthropicFormat(tools),
    stream: true
});
```

**Funciones helper necesarias:**
- `convertMessagesToAnthropicFormat()` - ‚úÖ Ya implementada
- `convertToolsToAnthropicFormat()` - Pendiente (convertir de Zod schemas a JSON Schema)
- Manejo del streaming response

### Opci√≥n B: Usar @ai-sdk/anthropic con Extended Prompts (M√°s simple)

Si @ai-sdk/anthropic soporta caching en versiones futuras, solo necesitar√°s:

```typescript
const result = streamText({
    model: anthropic('claude-sonnet-4-20250514'),
    experimental_telemetry: {
        isEnabled: true,
        recordInputs: true
    },
    system: systemPromptBlocks, // Pasar array directamente
    // ... resto igual
});
```

## üí∞ Ahorro Estimado

Con caching completo activo:

| Componente | Tokens | Sin Cache | Con Cache (read) | Ahorro |
|------------|--------|-----------|------------------|--------|
| System Prompt Est√°tico | ~2300 | $0.003/1K tokens | $0.0003/1K tokens | **90%** |
| System Prompt Din√°mico | ~200 | $0.003/1K tokens | $0.003/1K tokens | 0% |
| **Total por request** | ~2500 | ~$0.0075 | ~$0.0009 | **~88%** |

**Asumiendo 1000 requests/d√≠a:**
- Sin cache: $7.50/d√≠a = ~$225/mes
- Con cache: $0.90/d√≠a = ~$27/mes
- **Ahorro: ~$198/mes** (88%)

## üìä C√≥mo Verificar que Funciona

### 1. Headers de Response
Busca estos headers en la respuesta de Anthropic:

```
anthropic-ratelimit-tokens-remaining
anthropic-ratelimit-requests-remaining
x-anthropic-cache-creation-input-tokens  # Tokens escritos al cache
x-anthropic-cache-read-input-tokens      # Tokens le√≠dos del cache
```

### 2. Logs del Backend
Agrega logging despu√©s de cada call:

```typescript
const response = await anthropicClient.messages.create({...});

console.log('[CACHE METRICS]', {
    cacheCreationTokens: response.usage.cache_creation_input_tokens || 0,
    cacheReadTokens: response.usage.cache_read_input_tokens || 0,
    inputTokens: response.usage.input_tokens,
    outputTokens: response.usage.output_tokens
});
```

### 3. Patr√≥n Esperado
- **Primer request**: `cache_creation_input_tokens: ~2300`
- **Requests siguientes (dentro de 5 min)**: `cache_read_input_tokens: ~2300`
- **Despu√©s de 5 min**: Cache expira, vuelve a `cache_creation_input_tokens`

## üîß Estado Actual vs Completo

| Feature | Estado Actual | Con Caching Completo |
|---------|--------------|---------------------|
| SDK Nativo instalado | ‚úÖ | ‚úÖ |
| Prompt estructurado | ‚úÖ | ‚úÖ |
| Modelo Anthropic directo | ‚úÖ | ‚úÖ |
| cache_control en system | ‚ö†Ô∏è Preparado | ‚úÖ Activo |
| Ahorro de costos | ~20% (menos overhead OpenRouter) | ~88% |

## üìù Pr√≥ximos Pasos (Opcional)

1. Implementar `convertToolsToAnthropicFormat()` para convertir tools de Zod a JSON Schema
2. Reemplazar `streamText()` por `anthropicClient.messages.create()` en el chat principal
3. Adaptar el streaming response al formato esperado por el frontend
4. Monitorear headers de cache en logs
5. Validar ahorro de costos en el dashboard de Anthropic

## ‚ö° Quick Win Actual

Incluso sin caching completo, los cambios actuales dan:
- ‚úÖ Menos latencia (1 hop menos: directo a Anthropic vs OpenRouter ‚Üí Anthropic)
- ‚úÖ C√≥digo preparado para caching (solo cambiar a SDK nativo)
- ‚úÖ Prompt optimizado y mantenible (separado en bloques l√≥gicos)
